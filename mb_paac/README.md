# Intrinsic Motivation in Reinforcement Learning with Deep Dynamics Models
A model-based version of the open source reinforcement learning repo for [PAAC](https://github.com/Alfredvc/paac) from [(Clemente et al. 2017)](https://arxiv.org/abs/1705.04862)


## The Deep Dynamics Model
The proposed dynamics model consists of an autoencoder (AE) and a transition prediction model (TPM). 
First, the AE compresses the high-dimensional observation into a latent space.
Then, the TPM learns to predict state transitions in the latent space generated by the AE.
This is done by feeding the TPM with the latent variable of the previous state, the latent variable of the current state and the selected action.

The whole dynamics model is trained dynamically along with the model-free agent.

## Intrinsic Reward Bonuses
Four different intrinsic reward bonuses are tested:

- **Autoencoder loss (AL)** - generates high scores for states that results with a high reconstruction error.
- **Dynamics loss (DL)** - generates high scores for state transitions that results with a high prediction error from the TPM.
- **Dynamics uncertainty (DU)** - extracts the uncertainty from the TPM by using MC dropout. Generates high scores for high uncertainty.
- **Bootstrapped dynamics uncertainty (BDU)** - based on the same uncertainty as DU, but the TPM is bootstrapped (using multiple output heads)
